# llm-sql-generation
SQL Query Generation by Fine-tuning an LLM
## BigQuery Query Generation with Fine-tuned LLMs

This project explores the potential of fine-tuning large language models (LLMs) for automatically generating BigQuery queries from natural language descriptions. By leveraging the capabilities of LLMs, we aim to empower domain experts and non-technical stakeholders to interact with BigQuery data more intuitively and effectively.

## Project Overview

1. **Problem Statement:** Crafting BigQuery queries often requires a solid understanding of the data schema and the query language, posing a challenge for domain experts.
2. **Proposed Solution:** Fine-tune an open-source LLM to translate natural language descriptions of desired queries into accurate SQL queries for BigQuery.
3. **Benefits:**
    - Increased accessibility of BigQuery data for non-technical users.
    - Improved efficiency in query formulation.
    - Reduced reliance on data engineers for basic query generation.

## Implementation

1. **Dataset:** The model is fine-tuned on a dataset of natural language descriptions paired with corresponding BigQuery queries ([http://archive.ics.uci.edu/dataset/852/gender+gap+in+spanish+wp](http://archive.ics.uci.edu/dataset/852/gender+gap+in+spanish+wp)).
2. **Base LLM:** We use the Mistral 7b model (4-bit version) from  Transformers.
3. **Fine-tuning:** The model is fine-tuned using the SFTTrainer from the Transformers library with adjustments for memory usage and optimization.
4. **BigQuery Interaction:** The code demonstrates how to connect to BigQuery and execute the generated queries (commented out due to authentication issues).

## Results and Evaluation

**Note:** Due to time constraints and resource limitations, a comprehensive evaluation wasn't performed in this instance. However, the provided code outlines a potential approach for fine-tuning and testing the LLM.

A robust evaluation plan for general availability would involve:

1. **Accuracy:** Measuring the percentage of syntactically correct and semantically equivalent queries generated by the model on a held-out validation set.
2. **Generalizability:** Testing the model's performance on unseen data with varying question and context distributions to ensure it generalizes beyond the training data.
3. **Error Analysis:** Examining the model's errors to identify areas for improvement, such as retraining with more relevant data or refining the training process.
4. **A/B Testing (Optional):** Conducting user studies to compare the LLM's query generation capabilities with a traditional BigQuery interface.

## Bonus Tasks

1. **Regression with LLM:** While not implemented due to lack of data, the code provides a structure for fine-tuning the LLM to predict weightIJ based on textual descriptions of editor attributes.
2. **Model Evaluation Plan:** A detailed plan for evaluating the model's readiness for general availability is outlined in the Results and Evaluation section.

## Next Steps

1. Train the model on a larger dataset specifically focused on BigQuery query generation.
2. Implement a comprehensive evaluation plan to assess accuracy, generalizability, and user experience.
3. Integrate the LLM into a user-friendly interface for interacting with BigQuery.

## Conclusion

This project demonstrates the promising potential of fine-tuning LLMs for BigQuery query generation. By addressing the challenges of data accessibility and query formulation, this approach can empower a broader range of users to leverage the power of BigQuery data analysis.
